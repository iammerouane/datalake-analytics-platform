{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98213c7",
   "metadata": {},
   "source": [
    "# Comment charger les modèles de ML batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b304b",
   "metadata": {},
   "source": [
    "# 1. Analyse prédictive des ventes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1d615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------------+\n",
      "|Year|Month|Day|         features|\n",
      "+----+-----+---+-----------------+\n",
      "|2023|   10|  9|[2023.0,10.0,9.0]|\n",
      "+----+-----+---+-----------------+\n",
      "\n",
      "+----+-----+---+-----------------+-----------------+\n",
      "|Year|Month|Day|         features|       prediction|\n",
      "+----+-----+---+-----------------+-----------------+\n",
      "|2023|   10|  9|[2023.0,10.0,9.0]|25.45667985623163|\n",
      "+----+-----+---+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Créez une session Spark\n",
    "spark = SparkSession.builder.appName(\"Load_Models\").getOrCreate()\n",
    "\n",
    "# Charger le modèle\n",
    "loaded_model = LinearRegressionModel.load(\"hdfs://hadoop-namenode:9000/mllib_models/sales_prediction_model\")\n",
    "\n",
    "# Créer un DataFrame avec les caractéristiques pour la prédiction\n",
    "new_data = spark.createDataFrame([(2023, 10, 9)], [\"Year\", \"Month\", \"Day\"])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Year\", \"Month\", \"Day\"], outputCol=\"features\")\n",
    "\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "new_data.show()\n",
    "\n",
    "# Utiliser le modèle pour faire des prédictions\n",
    "predictions = loaded_model.transform(new_data)\n",
    "\n",
    "# Afficher les prédictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60aa37",
   "metadata": {},
   "source": [
    "# 2. Segmentation de la clientèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d040bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|CustomerKey|TotalPurchase|prediction|\n",
      "+-----------+-------------+----------+\n",
      "|         28|          507|         3|\n",
      "|         27|          186|         0|\n",
      "|         26|          712|         3|\n",
      "|         12|          234|         0|\n",
      "|         22|          602|         3|\n",
      "|          1|          377|         0|\n",
      "|         13|         1155|         2|\n",
      "|          6|          395|         0|\n",
      "|         16|         1419|         2|\n",
      "|          3|          394|         0|\n",
      "|         20|          644|         3|\n",
      "|          5|          391|         0|\n",
      "|         19|          374|         0|\n",
      "|         15|          246|         0|\n",
      "|          9|          657|         3|\n",
      "|         17|          624|         3|\n",
      "|          4|          504|         3|\n",
      "|          8|         2301|         1|\n",
      "|         23|          245|         0|\n",
      "|          7|          346|         0|\n",
      "+-----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#charger le modèle\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "loaded_model = KMeansModel.load(\"hdfs://hadoop-namenode:9000/mllib_models/customers_segmentation_model\")\n",
    "\n",
    "# Chargement des données depuis le fichier CSV\n",
    "orders_df = spark.read.csv(\"hdfs://hadoop-namenode:9000/orders.csv\", header=True, inferSchema=True)\n",
    "nouveaux_clients_df = spark.read.csv(\"hdfs://hadoop-namenode:9000/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Regroupement des données d'achat par client\n",
    "customer_purchase_data = orders_df.groupBy(\"CustomerKey\").sum(\"Quantity\").withColumnRenamed(\"sum(Quantity)\", \"TotalPurchase\")\n",
    "\n",
    "# Fusion des données de comportement d'achat avec les données clients\n",
    "nouveaux_clients_df = nouveaux_clients_df.join(customer_purchase_data, on=\"CustomerKey\", how=\"inner\")\n",
    "\n",
    "# Sélection des caractéristiques pertinentes pour la PCA\n",
    "feature_columns = [\"TotalPurchase\"]\n",
    "\n",
    "# Création d'un vecteur d'assemblage des caractéristiques\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "nouveaux_clients_pretraites = assembler.transform(nouveaux_clients_df)\n",
    "\n",
    "# Mise à l'échelle des caractéristiques (si nécessaire)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(nouveaux_clients_pretraites)\n",
    "nouveaux_clients_pretraites = scaler_model.transform(nouveaux_clients_pretraites)\n",
    "\n",
    "# Utilisation du modèle K-Means chargé pour prédire les clusters des nouveaux clients\n",
    "predictions = loaded_model.transform(nouveaux_clients_pretraites)\n",
    "\n",
    "# Affichage des résultats\n",
    "predictions.select(\"CustomerKey\", \"TotalPurchase\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c1b92",
   "metadata": {},
   "source": [
    "# 3. Recommandation de produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6aecaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|CustomerKey|     recommendations|\n",
      "+-----------+--------------------+\n",
      "|         12|[{96, 5.726271}, ...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Chargement du modèle\n",
    "loaded_model = ALSModel.load(\"hdfs://hadoop-namenode:9000/mllib_models/recommendation_system_model\")\n",
    "\n",
    "customers_df = spark.read.csv(\"hdfs://hadoop-namenode:9000/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "user_id = 12\n",
    "user_recs = loaded_model.recommendForUserSubset(customers_df.where(F.col(\"CustomerKey\") == user_id), 5)\n",
    "user_recs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4f411",
   "metadata": {},
   "source": [
    "# 4. Optimisation de la chaîne d’approvisionnement\n",
    "comme c'est un modèle de regression linéaire il faut juste faire comme le premier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334ebae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
